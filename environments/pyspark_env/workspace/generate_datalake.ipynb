{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b048119-9265-429b-a408-fbe259ffa5ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:50:29.265571Z",
     "iopub.status.busy": "2025-08-14T01:50:29.262146Z",
     "iopub.status.idle": "2025-08-14T01:50:29.284097Z",
     "shell.execute_reply": "2025-08-14T01:50:29.280097Z",
     "shell.execute_reply.started": "2025-08-14T01:50:29.265255Z"
    }
   },
   "source": [
    "# DATALAKE staging\n",
    "This script is responsible to read and clean data from Raw folder and write the result to datalake folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7daa1a0e-5e81-465e-8255-d6c8870df753",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T04:20:23.287311Z",
     "iopub.status.busy": "2025-08-14T04:20:23.286446Z",
     "iopub.status.idle": "2025-08-14T04:20:23.359722Z",
     "shell.execute_reply": "2025-08-14T04:20:23.356127Z",
     "shell.execute_reply.started": "2025-08-14T04:20:23.287231Z"
    }
   },
   "outputs": [],
   "source": [
    "# SET CONSTANTS\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a46c6916-c755-4446-833b-2c7290f61007",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T04:20:23.370695Z",
     "iopub.status.busy": "2025-08-14T04:20:23.369748Z",
     "iopub.status.idle": "2025-08-14T04:20:23.454091Z",
     "shell.execute_reply": "2025-08-14T04:20:23.451181Z",
     "shell.execute_reply.started": "2025-08-14T04:20:23.370608Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb44b1f9-be7e-4795-aadc-54481ababa1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T04:20:23.462309Z",
     "iopub.status.busy": "2025-08-14T04:20:23.461194Z",
     "iopub.status.idle": "2025-08-14T04:20:49.444265Z",
     "shell.execute_reply": "2025-08-14T04:20:49.438164Z",
     "shell.execute_reply.started": "2025-08-14T04:20:23.462229Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import locale\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, from_unixtime, col, to_date, sum, avg\n",
    "from pyspark.sql.types import DateType, TimestampType, StructType\n",
    "\n",
    "# Instantiate sparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GenerateDatalake\") \\\n",
    "    .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read raw schema and deserialize it\n",
    "with open('raw_schema.json', 'r') as f:\n",
    "    raw_schema_obj = StructType.fromJson(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5a85a8f-8947-4993-bc71-9efbc29189a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T04:20:49.455208Z",
     "iopub.status.busy": "2025-08-14T04:20:49.451633Z",
     "iopub.status.idle": "2025-08-14T04:20:49.534632Z",
     "shell.execute_reply": "2025-08-14T04:20:49.529620Z",
     "shell.execute_reply.started": "2025-08-14T04:20:49.455038Z"
    }
   },
   "outputs": [],
   "source": [
    "def etl_park_data(df):\n",
    "    SELECTED_FEATURES = [\n",
    "        col('name').alias('park_name'),\n",
    "        'extracted_at',\n",
    "        col('extracted_at').cast(TimestampType()).alias('extracted_at_time'),\n",
    "        to_date('extracted_at_time').alias('extracted_date'),\n",
    "        explode('liveData').alias('live_data')\n",
    "    ]\n",
    "    if DEBUG:\n",
    "        pre_dup = df.count()\n",
    "    df = df.drop_duplicates()\n",
    "    if DEBUG:\n",
    "        pos_dup = df.count()\n",
    "        print(\"Removed duplicates!\", f\"Count rows before: {pre_dup}\", f\"Count rows after: {pos_dup}\", sep = '\\n')\n",
    "    df_features = df.select(*SELECTED_FEATURES)\n",
    "    df_features = df_features.select(*[c+'.*' if c == 'live_data' else c for c in df_features.columns])\\\n",
    "                            .withColumnRenamed('entityType', 'entity_type')\\\n",
    "                            .where(\"entity_type != 'PARK' AND extracted_date IS NOT NULL\").cache()\n",
    "    if DEBUG:\n",
    "        print(\"Expanded live data!\", f\"Count rows now: {df_features.count()}\", \"Sample of data:\", sep = '\\n')\n",
    "        df_features.show(1, truncate=False, vertical=True)\n",
    "    return df_features\n",
    "\n",
    "\n",
    "def write_datalake(df, park_name):\n",
    "    path = f'./datalake_layer/{park_name}'\n",
    "    print(f'Writing DF to {path}')\n",
    "    df.write\\\n",
    "        .mode('overwrite')\\\n",
    "        .partitionBy('entity_type', 'extracted_date')\\\n",
    "        .orc(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de29c0c7-e7d1-45a0-be6b-38c7363c3823",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T04:20:49.545297Z",
     "iopub.status.busy": "2025-08-14T04:20:49.541051Z",
     "iopub.status.idle": "2025-08-14T04:51:21.312661Z",
     "shell.execute_reply": "2025-08-14T04:51:21.308619Z",
     "shell.execute_reply.started": "2025-08-14T04:20:49.545116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./raw_layer/park_animal_kingdom_theme_park\n",
      "Processing park 'Animal Kingdom', and generating their respective datalake, this might take a while!\n",
      "Writing DF to ./datalake_layer/animal_kingdom\n",
      "./raw_layer/park_epcot\n",
      "Processing park 'Epcot', and generating their respective datalake, this might take a while!\n",
      "Writing DF to ./datalake_layer/epcot\n",
      "./raw_layer/park_hollywood_studios\n",
      "Processing park 'Hollywood Studios', and generating their respective datalake, this might take a while!\n",
      "Writing DF to ./datalake_layer/hollywood_studios\n",
      "./raw_layer/park_magic_kingdom_park\n",
      "Processing park 'Magic Kingdom', and generating their respective datalake, this might take a while!\n",
      "Writing DF to ./datalake_layer/magic_kingdom\n",
      "./raw_layer/park_universal_epic_universe\n",
      "Processing park 'Universal Epic Universe', and generating their respective datalake, this might take a while!\n",
      "Writing DF to ./datalake_layer/universal_epic_universe\n",
      "./raw_layer/park_universal_islands_of_adventure\n",
      "Processing park 'Universal Islands Of Adventure', and generating their respective datalake, this might take a while!\n",
      "Writing DF to ./datalake_layer/universal_islands_of_adventure\n",
      "./raw_layer/park_universal_studios_florida\n",
      "Processing park 'Universal Studios Florida', and generating their respective datalake, this might take a while!\n",
      "Writing DF to ./datalake_layer/universal_studios_florida\n"
     ]
    }
   ],
   "source": [
    "for park in glob('./raw_layer/*'):\n",
    "    park_name = park.rpartition('/')[-1]\n",
    "    park_name = re.sub(r'[_]*(park|theme)[_]*', '', park_name)\n",
    "    park_name_harmonized = ' '.join([word.capitalize() for word in park_name.split('_')])\n",
    "    print(park)\n",
    "    print(f\"Processing park '{park_name_harmonized}', and generating their respective datalake, this might take a while!\")\n",
    "\n",
    "    \n",
    "    df = spark.read.schema(raw_schema_obj).json(park+'/**')\n",
    "    # TODO filter only new partitions\n",
    "\n",
    "    df_transformed = etl_park_data(df)\n",
    "    write_datalake(df_transformed, park_name)\n",
    "    \n",
    "    df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6ed202c-f4bd-4ee0-a8b3-fea8c0a2f6db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T14:43:39.087114Z",
     "iopub.status.busy": "2025-08-14T14:43:39.086255Z",
     "iopub.status.idle": "2025-08-14T14:43:39.153729Z",
     "shell.execute_reply": "2025-08-14T14:43:39.150986Z",
     "shell.execute_reply.started": "2025-08-14T14:43:39.087072Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finish session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05484d23-6c2c-4c9f-838f-d0cf3ce597f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
