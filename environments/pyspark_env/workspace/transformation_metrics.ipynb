{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21566461-a179-4df7-8621-803834840d6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:50:29.265571Z",
     "iopub.status.busy": "2025-08-14T01:50:29.262146Z",
     "iopub.status.idle": "2025-08-14T01:50:29.284097Z",
     "shell.execute_reply": "2025-08-14T01:50:29.280097Z",
     "shell.execute_reply.started": "2025-08-14T01:50:29.265255Z"
    }
   },
   "source": [
    "# Transformation metrics\n",
    "This script is responsible create all metrics necessary and upload them to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df6a8586-716f-479a-9ed2-505e0361fc87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T20:32:50.181554Z",
     "iopub.status.busy": "2025-08-18T20:32:50.178288Z",
     "iopub.status.idle": "2025-08-18T20:32:50.242948Z",
     "shell.execute_reply": "2025-08-18T20:32:50.239716Z",
     "shell.execute_reply.started": "2025-08-18T20:32:50.181446Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "0bf447ba-f45d-4867-a2c2-6760b2cdbaf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T22:05:57.431733Z",
     "iopub.status.busy": "2025-08-20T22:05:57.408621Z",
     "iopub.status.idle": "2025-08-20T22:05:58.178335Z",
     "shell.execute_reply": "2025-08-20T22:05:58.167417Z",
     "shell.execute_reply.started": "2025-08-20T22:05:57.431199Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'timestamp_diff' from 'pyspark.sql.functions' (/usr/local/spark/python/pyspark/sql/functions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[446], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession, Window\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkConf\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m explode, from_unixtime, col, to_date, \u001b[38;5;28msum\u001b[39m, avg, udf, timestamp_diff\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DateType, TimestampType, StructType, DoubleType, StructField, StringType\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprophet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Prophet\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'timestamp_diff' from 'pyspark.sql.functions' (/usr/local/spark/python/pyspark/sql/functions.py)"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.functions import explode, from_unixtime, col, to_date, sum, avg, udf, timestamp_diff\n",
    "from pyspark.sql.types import DateType, TimestampType, StructType, DoubleType, StructField, StringType\n",
    "from prophet import Prophet\n",
    "\n",
    "from glob import glob\n",
    "import requests\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import locale\n",
    "import os\n",
    "\n",
    "DB_URL = \"jdbc:postgresql://postgres:5432/themeparkwizard\"\n",
    "PROPERTIES_CUSTOM = {\"user\": os.environ['POSTGRES_USER'],\"password\": os.environ['POSTGRES_PASSWORD'], \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MetricBuilder\") \\\n",
    "    .config(\"spark.jars\", \"jars/postgresql-42.7.7.jar\") \\\n",
    "    .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "7d074a6c-dac5-42f3-83d8-3cc02c3e8b92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T22:54:43.071519Z",
     "iopub.status.busy": "2025-08-20T22:54:43.070428Z",
     "iopub.status.idle": "2025-08-20T22:54:43.161508Z",
     "shell.execute_reply": "2025-08-20T22:54:43.150499Z",
     "shell.execute_reply.started": "2025-08-20T22:54:43.071438Z"
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=TimestampType())\n",
    "def min_hour(data):\n",
    "    if isinstance(data, list):\n",
    "        for ee in data:\n",
    "            if ee.type == 'Early Entry':\n",
    "                return ee.startTime\n",
    "\n",
    "@udf(returnType=TimestampType())\n",
    "def max_hour(data):\n",
    "    if isinstance(data, list):\n",
    "        for op in data:\n",
    "            if op.type == 'Operating':\n",
    "                return op.endTime\n",
    "\n",
    "def save_into_postgres(df, table, mode):\n",
    "    df.write.jdbc(url=DB_URL, table=table, mode=mode, properties=PROPERTIES_CUSTOM)\n",
    "\n",
    "def agg_avg_time_compute():\n",
    "    print('Computing agg_avg_time...')\n",
    "    result_avg = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        extracted_date,\n",
    "        id as entity_id, \n",
    "        AVG(queue.STANDBY.waitTime) AS avg_standby_waittime\n",
    "    FROM datalake_table\n",
    "    WHERE entity_type = 'ATTRACTION' AND queue.STANDBY.waitTime is not null\n",
    "    GROUP BY 1, 2\n",
    "    ORDER BY 1\n",
    "    \"\"\")\n",
    "    result_avg.printSchema()\n",
    "    save_into_postgres(result_avg, \"themeparkwizard.agg_avg_time\", 'append')\n",
    "    # result_avg.write.jdbc(url=DB_URL, table=\"themeparkwizard.agg_avg_time\", mode='append', properties=PROPERTIES_CUSTOM)\n",
    "\n",
    "# def operating_ratio_compute(): \n",
    "#     print('Computing operating ratio by day...')\n",
    "#     result_ratio = spark.sql(\"\"\"\n",
    "#     WITH lag_status AS (\n",
    "#         SELECT\n",
    "#             extracted_date,\n",
    "#             extracted_at_time,\n",
    "#             start_time,\n",
    "#             end_time,\n",
    "#             id as entity_id,\n",
    "#             status,\n",
    "#             lag(status, 1) OVER (PARTITION BY id, start_time, end_time ORDER BY extracted_at_time) as prev_status\n",
    "#         FROM datalake_table\n",
    "#         WHERE entity_type = 'ATTRACTION'\n",
    "#         ORDER BY 5, 2\n",
    "#     ),\n",
    "#     time_diff_operational AS (\n",
    "#         SELECT\n",
    "#             extracted_date,\n",
    "#             extracted_at_time,\n",
    "#             start_time,\n",
    "#             end_time,\n",
    "#             entity_id,\n",
    "#             status,\n",
    "#             prev_status,\n",
    "#             lead(extracted_at_time, 1) OVER (PARTITION BY entity_id ORDER BY extracted_at_time) as next_time,\n",
    "#             CASE WHEN status <> 'OPERATING' OR status IS NULL THEN\n",
    "#                 CASE WHEN next_time IS NOT NULL THEN next_time - extracted_at_time\n",
    "#                 ELSE end_time - extracted_at_time\n",
    "#                 END\n",
    "#             ELSE INTERVAL '0 DAY'\n",
    "#             END AS diff_op\n",
    "#         FROM lag_status\n",
    "#         WHERE (status = 'OPERATING' and prev_status <> 'OPERATING') or (status <> 'OPERATING' and prev_status = 'OPERATING')\n",
    "#         order by entity_id, extracted_at_time\n",
    "#     ),\n",
    "#     dates AS (\n",
    "#         SELECT \n",
    "#             id as entity_id,\n",
    "#             explode(sequence(to_date(min(extracted_date)), to_date(max(extracted_date)))) as extracted_date\n",
    "#         FROM datalake_table\n",
    "#         GROUP BY 1\n",
    "#     ),\n",
    "#     pre_compute_amount AS (\n",
    "#         SELECT\n",
    "#             d.extracted_date,\n",
    "#             MIN(COALESCE(extracted_at_time, CAST(d.extracted_date as timestamp))) as extracted_ts,\n",
    "#             d.extracted_date + 1 as next_date,\n",
    "#             d.entity_id,\n",
    "#             status,\n",
    "#             prev_status,\n",
    "#             next_time,\n",
    "#             CASE\n",
    "#             WHEN status = 'OPERATING' THEN\n",
    "#                 CASE \n",
    "#                     WHEN prev_status <> 'OPERATING' AND to_date(next_time) = d.extracted_date THEN \n",
    "#                         SUM(diff_op)\n",
    "#                     ELSE extracted_ts - CAST(d.extracted_date as timestamp)\n",
    "#                 END\n",
    "#             ELSE \n",
    "#                 CASE \n",
    "#                     WHEN prev_status = 'OPERATING' AND to_date(next_time) = d.extracted_date THEN \n",
    "#                         SUM(diff_op)\n",
    "#                     ELSE CAST(next_date as timestamp) - extracted_ts\n",
    "#                 END\n",
    "#             END as closed_by_day,\n",
    "#             CASE\n",
    "#             WHEN SUM(diff_op) - closed_by_day >= INTERVAL '0 SECOND' THEN\n",
    "#                 SUM(diff_op)\n",
    "#             ELSE INTERVAL '0 SECOND'\n",
    "#             END as sum_time_not_operating\n",
    "#         FROM time_diff_operational tdo\n",
    "#         RIGHT JOIN dates d\n",
    "#         ON tdo.extracted_date = d.extracted_date AND d.entity_id = tdo.entity_id\n",
    "#         GROUP BY 1,3, 4, 5,6,7\n",
    "#         ORDER BY 1,2,3,4\n",
    "#     ),\n",
    "#     second_pre_compute AS (\n",
    "#         SELECT \n",
    "#             entity_id,\n",
    "#             extracted_date,\n",
    "#             next_date,\n",
    "#             closed_by_day,\n",
    "#             status,\n",
    "#             sum_time_not_operating,\n",
    "#             SUM(sum_time_not_operating - closed_by_day) over (partition by entity_id order by extracted_date RANGE BETWEEN UNBOUNDED PRECEDING AND -1 FOLLOWING) as cumsum,\n",
    "#             CASE\n",
    "#                 WHEN cumsum = INTERVAL '0 DAY' THEN\n",
    "#                     INTERVAL '0 DAY'\n",
    "#                 ELSE closed_by_day\n",
    "#             END AS new_closed_by_day\n",
    "#         FROM pre_compute_amount\n",
    "#         GROUP BY 1,2,3,4,5,6\n",
    "#         ORDER BY entity_id\n",
    "        \n",
    "#     )\n",
    "#     SELECT \n",
    "#         entity_id,\n",
    "#         extracted_date,\n",
    "#         new_closed_by_day,\n",
    "#         status,\n",
    "#         sum_time_not_operating,\n",
    "#         cumsum,\n",
    "#         CASE\n",
    "#         WHEN cumsum < INTERVAL '1 DAY' and cumsum >= INTERVAL '0 DAY' THEN\n",
    "#             CASE \n",
    "#             WHEN status = 'OPERATING' THEN\n",
    "#                 new_closed_by_day\n",
    "#             ELSE new_closed_by_day - cumsum\n",
    "#             END\n",
    "#         ELSE closed_by_day\n",
    "#         END AS final_sum\n",
    "#     FROM second_pre_compute\n",
    "#     ORDER BY entity_id\n",
    "#     \"\"\")\n",
    "#     result_ratio.printSchema()\n",
    "#     result_ratio.show(70, truncate=False)\n",
    "\n",
    "#     spark.sql(\"\"\"\n",
    "#     WITH lag_status AS (\n",
    "#         SELECT\n",
    "#             extracted_date,\n",
    "#             extracted_at_time,\n",
    "#             start_time,\n",
    "#             end_time,\n",
    "#             id as entity_id,\n",
    "#             status,\n",
    "#             lag(status, 1) OVER (PARTITION BY id, start_time, end_time ORDER BY extracted_at_time) as prev_status\n",
    "#         FROM datalake_table\n",
    "#         WHERE entity_type = 'ATTRACTION'\n",
    "#         ORDER BY 5, 2\n",
    "#     )\n",
    "#         SELECT\n",
    "#             extracted_date,\n",
    "#             extracted_at_time,\n",
    "#             start_time,\n",
    "#             end_time,\n",
    "#             entity_id,\n",
    "#             status,\n",
    "#             prev_status,\n",
    "#             lead(extracted_at_time, 1) OVER (PARTITION BY entity_id ORDER BY extracted_at_time) as next_time,\n",
    "#             CASE \n",
    "#             WHEN status <> 'OPERATING' THEN\n",
    "#                 CASE WHEN next_time IS NOT NULL THEN next_time - extracted_at_time\n",
    "#                 ELSE end_time - extracted_at_time\n",
    "#                 END\n",
    "#             ELSE INTERVAL '0 DAY'\n",
    "#             END AS diff_op\n",
    "#         FROM lag_status\n",
    "#         WHERE (status = 'OPERATING' and prev_status <> 'OPERATING') or (status <> 'OPERATING' and prev_status = 'OPERATING')\n",
    "#         order by entity_id, extracted_at_time\"\"\").show(40, truncate=False)\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# SUM(diff_op) OVER (PARTITION BY d.extracted_date ORDER BY extracted_at_time) as sum_time_not_operating\n",
    "\n",
    "    #--unix_timestamp(next_time) - unix_timestamp(extracted_at_time)\n",
    "    #--unix_timestamp(end_time) - unix_timestamp(extracted_at_time)\n",
    "#     WITH lag_status AS (\n",
    "# unix_timestamp(extracted_at_time) - lag(unix_timestamp(extracted_at_time), 1) OVER (PARTITION BY entity_id, start_time, end_time ORDER BY extracted_at_time) as diff_op\n",
    "                #     WHEN end_time = next_end_time THEN\n",
    "                #     unix_timestamp(next_time) - unix_timestamp(extracted_at_time)\n",
    "                # WHEN end_time <> next_end_time THEN\n",
    "                #     CASE \n",
    "                #     WHEN status = 'OPERATING' THEN\n",
    "                #         unix_timestamp(end_time) - unix_timestamp(extracted_at_time)\n",
    "                #     ELSE\n",
    "                #         unix_timestamp(next_time) - unix_timestamp(extracted_at_time)\n",
    "# ),\n",
    "\n",
    "        # --time_diff_operational AS (\n",
    "#     time_diff_operational AS (\n",
    "#         SELECT\n",
    "#             extracted_date,\n",
    "#             extracted_at_time,\n",
    "#             start_time,\n",
    "#             end_time,\n",
    "#             entity_id,\n",
    "#             unix_timestamp(extracted_at_time) - lag(unix_timestamp(extracted_at_time), 1) OVER (PARTITION BY entity_id, start_time, end_time ORDER BY extracted_at_time) as diff_op\n",
    "#         FROM lag_status\n",
    "#         WHERE status = 'OPERATING'\n",
    "#     )\n",
    "#     SELECT\n",
    "#         to_date(start_time) as extracted_date,\n",
    "#         entity_id,\n",
    "#         COALESCE(SUM(diff_op)/60, 0) AS sum_time_not_operating\n",
    "#     FROM time_diff_operational\n",
    "#     GROUP BY 1,2\n",
    "#     ORDER BY 2,1\n",
    "\n",
    "    \n",
    "    # result_ratio.printSchema()\n",
    "    # save_into_postgres(result_ratio, \"themeparkwizard.amount_not_operating_by_day\", 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "53cf81e2-0120-477e-ba84-fb97e2bc7fac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T22:54:43.459154Z",
     "iopub.status.busy": "2025-08-20T22:54:43.455784Z",
     "iopub.status.idle": "2025-08-20T22:54:43.495791Z",
     "shell.execute_reply": "2025-08-20T22:54:43.491074Z",
     "shell.execute_reply.started": "2025-08-20T22:54:43.459038Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_dl = spark.read.orc('datalake_layer/epcot').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "da44bfbc-4aa8-46d6-ab20-f92c1079f784",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T22:54:43.940132Z",
     "iopub.status.busy": "2025-08-20T22:54:43.938457Z",
     "iopub.status.idle": "2025-08-20T22:54:43.960768Z",
     "shell.execute_reply": "2025-08-20T22:54:43.956895Z",
     "shell.execute_reply.started": "2025-08-20T22:54:43.940047Z"
    }
   },
   "outputs": [],
   "source": [
    "# operating_ratio_compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "abf6ff69-b9cf-4a94-9fd3-482e36b75cbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T22:54:52.663312Z",
     "iopub.status.busy": "2025-08-20T22:54:52.661946Z",
     "iopub.status.idle": "2025-08-20T22:55:00.271706Z",
     "shell.execute_reply": "2025-08-20T22:55:00.264378Z",
     "shell.execute_reply.started": "2025-08-20T22:54:52.663233Z"
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"extracted_date\", DateType(), False),\n",
    "    StructField(\"entity_id\", StringType(), False),\n",
    "    StructField(\"avg_standby_waittime\", DoubleType(), False),\n",
    "])\n",
    "save_into_postgres(spark.createDataFrame([], schema), \"themeparkwizard.agg_avg_time\", 'overwrite')\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"extracted_date\", DateType(), False),\n",
    "    StructField(\"entity_id\", StringType(), False),\n",
    "    StructField(\"sum_time_not_operating\", DoubleType(), False),\n",
    "])\n",
    "save_into_postgres(spark.createDataFrame([], schema), \"themeparkwizard.amount_not_operating_by_day\", 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "240ac968-85c6-43fd-a8bb-71f1cb85feb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T18:32:21.382479Z",
     "iopub.status.busy": "2025-08-20T18:32:21.378796Z",
     "iopub.status.idle": "2025-08-20T18:32:24.307692Z",
     "shell.execute_reply": "2025-08-20T18:32:24.303423Z",
     "shell.execute_reply.started": "2025-08-20T18:32:21.382273Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load dim_park_entity\n",
    "df_parks = spark.read.json('general_schemas_tables/park_by_entity_meta_new.json')\n",
    "df_parks.write.jdbc(url=DB_URL, table=f\"themeparkwizard.dim_park_entity\", mode='overwrite', properties=PROPERTIES_CUSTOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "15cdad2a-d5c9-4265-a2de-c3e1ab1230bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T22:55:00.283898Z",
     "iopub.status.busy": "2025-08-20T22:55:00.282241Z",
     "iopub.status.idle": "2025-08-20T23:07:06.617969Z",
     "shell.execute_reply": "2025-08-20T23:07:06.609572Z",
     "shell.execute_reply.started": "2025-08-20T22:55:00.283795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming datalake_layer/animal_kingdom ...\n",
      "Computing agg_avg_time...\n",
      "root\n",
      " |-- extracted_date: date (nullable = true)\n",
      " |-- entity_id: string (nullable = true)\n",
      " |-- avg_standby_waittime: double (nullable = true)\n",
      "\n",
      "Transforming datalake_layer/epcot ...\n",
      "Computing agg_avg_time...\n",
      "root\n",
      " |-- extracted_date: date (nullable = true)\n",
      " |-- entity_id: string (nullable = true)\n",
      " |-- avg_standby_waittime: double (nullable = true)\n",
      "\n",
      "Transforming datalake_layer/hollywood_studios ...\n",
      "Computing agg_avg_time...\n",
      "root\n",
      " |-- extracted_date: date (nullable = true)\n",
      " |-- entity_id: string (nullable = true)\n",
      " |-- avg_standby_waittime: double (nullable = true)\n",
      "\n",
      "Transforming datalake_layer/magic_kingdom ...\n",
      "Computing agg_avg_time...\n",
      "root\n",
      " |-- extracted_date: date (nullable = true)\n",
      " |-- entity_id: string (nullable = true)\n",
      " |-- avg_standby_waittime: double (nullable = true)\n",
      "\n",
      "Transforming datalake_layer/universal_epic_universe ...\n",
      "Computing agg_avg_time...\n",
      "root\n",
      " |-- extracted_date: date (nullable = true)\n",
      " |-- entity_id: string (nullable = true)\n",
      " |-- avg_standby_waittime: double (nullable = true)\n",
      "\n",
      "Transforming datalake_layer/universal_islands_of_adventure ...\n",
      "Computing agg_avg_time...\n",
      "root\n",
      " |-- extracted_date: date (nullable = true)\n",
      " |-- entity_id: string (nullable = true)\n",
      " |-- avg_standby_waittime: double (nullable = true)\n",
      "\n",
      "Transforming datalake_layer/universal_studios_florida ...\n",
      "Computing agg_avg_time...\n",
      "root\n",
      " |-- extracted_date: date (nullable = true)\n",
      " |-- entity_id: string (nullable = true)\n",
      " |-- avg_standby_waittime: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for path in glob('datalake_layer/*'):\n",
    "    print(f'Transforming {path} ...')\n",
    "    df_dl = spark.read.orc(path).cache()\n",
    "    # df_dl_working_hour = df_dl.withColumn('start_time', min_hour(col('operatingHours')))\\\n",
    "    #                 .withColumn('end_time', max_hour(col('operatingHours')))\\\n",
    "    #                 .filter(col('extracted_at_time').between(col('start_time'), col('end_time')))\n",
    "    # df_dl_working_hour.createOrReplaceTempView('datalake_table_working_hour')\n",
    "    df_dl.createOrReplaceTempView('datalake_table')\n",
    "    agg_avg_time_compute()\n",
    "    #operating_ratio_compute()\n",
    "# df_dl.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ddd544-00f3-496c-970b-5506dfae04d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "cac78785-e030-43ce-b658-f9283cf8068d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T22:37:36.817873Z",
     "iopub.status.busy": "2025-08-20T22:37:36.816132Z",
     "iopub.status.idle": "2025-08-20T22:41:56.308309Z",
     "shell.execute_reply": "2025-08-20T22:41:56.303106Z",
     "shell.execute_reply.started": "2025-08-20T22:37:36.817811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+\n",
      "|  extracted_at_time|waitTime|\n",
      "+-------------------+--------+\n",
      "|2025-08-17 02:04:42|      60|\n",
      "|2025-08-17 01:59:42|      60|\n",
      "|2025-08-17 01:54:42|      60|\n",
      "|2025-08-17 01:49:42|      70|\n",
      "|2025-08-17 01:44:42|      70|\n",
      "|2025-08-17 01:39:42|      80|\n",
      "|2025-08-17 01:34:42|      80|\n",
      "|2025-08-17 01:29:42|      80|\n",
      "|2025-08-17 01:24:41|      80|\n",
      "|2025-08-17 01:19:42|      80|\n",
      "|2025-08-17 01:14:42|      80|\n",
      "|2025-08-17 01:09:41|      80|\n",
      "|2025-08-17 01:04:41|      80|\n",
      "|2025-08-17 00:59:41|      80|\n",
      "|2025-08-17 00:54:41|      80|\n",
      "|2025-08-17 00:49:41|      80|\n",
      "|2025-08-17 00:44:41|      80|\n",
      "|2025-08-17 00:39:41|      80|\n",
      "|2025-08-17 00:34:41|      80|\n",
      "|2025-08-17 00:29:41|      90|\n",
      "|2025-08-17 00:24:41|      90|\n",
      "|2025-08-17 00:19:41|      90|\n",
      "|2025-08-17 00:14:41|      90|\n",
      "|2025-08-17 00:09:40|      85|\n",
      "|2025-08-17 00:04:40|      85|\n",
      "|2025-08-16 23:59:40|      85|\n",
      "|2025-08-16 23:54:40|      85|\n",
      "|2025-08-16 23:49:40|      85|\n",
      "|2025-08-16 23:44:40|      85|\n",
      "|2025-08-16 23:39:39|      85|\n",
      "|2025-08-16 23:34:40|      85|\n",
      "|2025-08-16 23:29:40|      85|\n",
      "|2025-08-16 23:24:40|      85|\n",
      "|2025-08-16 23:19:40|      75|\n",
      "|2025-08-16 23:14:40|      75|\n",
      "|2025-08-16 23:09:40|      75|\n",
      "|2025-08-16 23:04:40|      75|\n",
      "|2025-08-16 22:59:40|      75|\n",
      "|2025-08-16 22:54:39|      75|\n",
      "|2025-08-16 22:49:39|      75|\n",
      "+-------------------+--------+\n",
      "only showing top 40 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5a43d1a7-ad53-4d25-abfe-25625f0da304\n",
    "df_magic = spark.read.orc('datalake_layer/magic_kingdom').where(\"id = '5a43d1a7-ad53-4d25-abfe-25625f0da304'\").select('extracted_at_time', 'queue.STANDBY.waitTime')\\\n",
    "                        .orderBy('extracted_at_time', ascending=False).show(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51f94fa-e2ad-440b-b4d1-bde44d8dc8a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54505c82-41a7-4439-b0f5-49cd6295fbd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d86a1bb-0939-429c-a587-ae793016e4a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T03:01:36.208450Z",
     "iopub.status.busy": "2025-08-17T03:01:36.205129Z",
     "iopub.status.idle": "2025-08-17T03:02:32.917536Z",
     "shell.execute_reply": "2025-08-17T03:02:32.914021Z",
     "shell.execute_reply.started": "2025-08-17T03:01:36.208336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- extracted_date: date (nullable = true)\n",
      " |-- attraction_name: string (nullable = true)\n",
      " |-- avg_standby_waittime: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dl.createOrReplaceTempView('datalake_table')\n",
    "test_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    extracted_date,\n",
    "    name AS attraction_name, \n",
    "    AVG(queue.STANDBY.waitTime) AS avg_standby_waittime\n",
    "FROM datalake_table\n",
    "WHERE entity_type = 'ATTRACTION' AND queue.STANDBY.waitTime is not null\n",
    "GROUP BY 1, 2\n",
    "ORDER BY 1\n",
    "\"\"\")\n",
    "test_df.printSchema()\n",
    "test_df.write.jdbc(url=DB_URL, table=\"themeparkwizard.agg_avg_time_epcot\", mode='overwrite', properties=PROPERTIES_CUSTOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9adaff2-289b-4c74-9a7e-2136fb8b3091",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T21:29:19.171198Z",
     "iopub.status.busy": "2025-08-13T21:29:19.170361Z",
     "iopub.status.idle": "2025-08-13T21:30:35.843936Z",
     "shell.execute_reply": "2025-08-13T21:30:35.840379Z",
     "shell.execute_reply.started": "2025-08-13T21:29:19.171131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+\n",
      "|                 ds|waitTime|\n",
      "+-------------------+--------+\n",
      "|2025-07-25 12:35:04|      30|\n",
      "|2025-07-25 12:40:04|      30|\n",
      "|2025-07-25 12:45:06|      30|\n",
      "|2025-07-25 12:50:05|      45|\n",
      "|2025-07-25 12:55:08|      45|\n",
      "|2025-07-25 13:00:05|      45|\n",
      "|2025-07-25 13:05:05|      60|\n",
      "|2025-07-25 13:10:05|      70|\n",
      "|2025-07-25 13:15:05|      80|\n",
      "|2025-07-25 13:20:04|      80|\n",
      "|2025-07-25 13:25:05|      80|\n",
      "|2025-07-25 13:30:04|      80|\n",
      "|2025-07-25 13:35:05|      80|\n",
      "|2025-07-25 13:40:05|      80|\n",
      "|2025-07-25 13:45:04|      70|\n",
      "|2025-07-25 13:50:04|      70|\n",
      "|2025-07-25 13:55:04|      70|\n",
      "|2025-07-25 14:00:06|      70|\n",
      "|2025-07-25 14:05:04|      70|\n",
      "|2025-07-25 14:10:05|      70|\n",
      "+-------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dl_to_model = df_dl.where(\"name == 'Guardians of the Galaxy: Cosmic Rewind' AND queue.STANDBY.waitTime is not null\")\\\n",
    "                        .select('extracted_at_time', 'queue.STANDBY.waitTime')\\\n",
    "                        .orderBy('extracted_at_time')\\\n",
    "                        .withColumnRenamed('extracted_at_time', 'ds')\\\n",
    "                        .withColumnRenamed('queue.STANDBY.waitTime', 'y')\n",
    "df_dl_to_model.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8637cabc-4732-4dbd-be62-4042fc6aacb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T21:31:09.192866Z",
     "iopub.status.busy": "2025-08-13T21:31:09.192182Z",
     "iopub.status.idle": "2025-08-13T21:31:09.828550Z",
     "shell.execute_reply": "2025-08-13T21:31:09.826475Z",
     "shell.execute_reply.started": "2025-08-13T21:31:09.192839Z"
    }
   },
   "outputs": [
    {
     "ename": "PySparkValueError",
     "evalue": "[CANNOT_CONVERT_COLUMN_INTO_BOOL] Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkValueError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Prophet()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_dl_to_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/prophet/forecaster.py:1220\u001b[0m, in \u001b[0;36mProphet.fit\u001b[0;34m(self, df, **kwargs)\u001b[0m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1217\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProphet object can only be fit once. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1218\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInstantiate a new object.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1220\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1221\u001b[0m initial_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_initial_params(model_inputs\u001b[38;5;241m.\u001b[39mK)\n\u001b[1;32m   1223\u001b[0m dat \u001b[38;5;241m=\u001b[39m dataclasses\u001b[38;5;241m.\u001b[39masdict(model_inputs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/prophet/forecaster.py:1128\u001b[0m, in \u001b[0;36mProphet.preprocess\u001b[0;34m(self, df, **kwargs)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, df: pd\u001b[38;5;241m.\u001b[39mDataFrame, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ModelInputData:\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;124;03m    Reformats historical data, standardizes y and extra regressors, sets seasonalities and changepoints.\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \n\u001b[1;32m   1125\u001b[0m \u001b[38;5;124;03m    Saves the preprocessed data to the instantiated object, and also returns the relevant components\u001b[39;00m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;124;03m    as a ModelInputData object.\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df):\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1130\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataframe must have columns \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m with the dates and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1131\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalues respectively.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1132\u001b[0m         )\n\u001b[1;32m   1133\u001b[0m     history \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnotnull()]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/column.py:1400\u001b[0m, in \u001b[0;36mColumn.__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__nonzero__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m   1401\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_CONVERT_COLUMN_INTO_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1402\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m   1403\u001b[0m     )\n",
      "\u001b[0;31mPySparkValueError\u001b[0m: [CANNOT_CONVERT_COLUMN_INTO_BOOL] Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions."
     ]
    }
   ],
   "source": [
    "model = Prophet()\n",
    "model.fit(df_dl_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdeaf5a-3c7e-43bc-968d-368a162d5f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ae35efe-d369-4439-a179-b224245d3a73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T04:08:16.240412Z",
     "iopub.status.busy": "2025-08-12T04:08:16.239139Z",
     "iopub.status.idle": "2025-08-12T04:08:16.252977Z",
     "shell.execute_reply": "2025-08-12T04:08:16.250506Z",
     "shell.execute_reply.started": "2025-08-12T04:08:16.240364Z"
    }
   },
   "outputs": [],
   "source": [
    "INTERVAL_OF_MINUTES = 60 * 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "61eac097-57c7-4704-81ca-63f99927b253",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T03:17:18.441885Z",
     "iopub.status.busy": "2025-08-13T03:17:18.441007Z",
     "iopub.status.idle": "2025-08-13T03:17:37.230231Z",
     "shell.execute_reply": "2025-08-13T03:17:37.227439Z",
     "shell.execute_reply.started": "2025-08-13T03:17:18.441844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+---------------+----------+-------------+----------------+\n",
      "|extracted_date|time_of_the_day|attraction_name|party_size|avg_wait_time|stddev_wait_time|\n",
      "+--------------+---------------+---------------+----------+-------------+----------------+\n",
      "+--------------+---------------+---------------+----------+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rest_df = spark.sql(f\"\"\"\n",
    "WITH wait_by_party AS (\n",
    "    SELECT\n",
    "        extracted_date,\n",
    "        date_format(cast(floor(try_divide(extracted_at, {INTERVAL_OF_MINUTES}))*{INTERVAL_OF_MINUTES} as timestamp), 'HH:mm') as time_of_the_day, --extracted_date,\n",
    "        name AS attraction_name,\n",
    "        CASE \n",
    "        WHEN da_exp.partySize <= 2 THEN\n",
    "            'Small group (<= 2)'\n",
    "        WHEN da_exp.partySize > 2 AND da_exp.partySize <= 4 THEN\n",
    "            'Medium group (3 and 4)'\n",
    "        WHEN da_exp.partySize > 4 AND da_exp.partySize <= 6 THEN\n",
    "            'Medium group (5 ant 6)'\n",
    "        WHEN da_exp.partySize > 6 THEN\n",
    "            'Big group (> 6)'\n",
    "        END as party_size,\n",
    "        COALESCE(AVG(da_exp.waitTime), 0) as avg_wait_time,\n",
    "        STDDEV(da_exp.waitTime) AS stddev_wait_time\n",
    "    FROM datalake_table\n",
    "    LATERAL VIEW EXPLODE(diningAvailability) as da_exp\n",
    "    WHERE entity_type = 'SHOW' --AND name = 'Garden Grill Restaurant'\n",
    "    GROUP BY 1,2,3,4\n",
    ")\n",
    "SELECT \n",
    "    *\n",
    "FROM wait_by_party\n",
    "ORDER BY 1, 2, 3, 4\n",
    "\"\"\").show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "15293ff6-31b4-4250-9bcc-0c1843c526be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T04:08:16.966919Z",
     "iopub.status.busy": "2025-08-12T04:08:16.965480Z",
     "iopub.status.idle": "2025-08-12T04:08:52.421608Z",
     "shell.execute_reply": "2025-08-12T04:08:52.415459Z",
     "shell.execute_reply.started": "2025-08-12T04:08:16.966861Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- extracted_date: date (nullable = true)\n",
      " |-- time_of_the_day: string (nullable = true)\n",
      " |-- attraction_name: string (nullable = true)\n",
      " |-- party_size: string (nullable = true)\n",
      " |-- avg_wait_time: double (nullable = false)\n",
      " |-- stddev_wait_time: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rest_df.printSchema()\n",
    "rest_df.write.jdbc(url=DB_URL, table=\"themeparkwizard.restaurant_wait_time_epcot\", mode='overwrite', properties=PROPERTIES_CUSTOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4fdfeb-9dc6-4c86-8bfc-9705632c1b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function = 4**(10/r)*q*d/AVG(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "bf079f23-2df0-4b5e-96f0-d6bf945dea13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T21:53:10.515794Z",
     "iopub.status.busy": "2025-08-20T21:53:10.499226Z",
     "iopub.status.idle": "2025-08-20T21:53:10.678240Z",
     "shell.execute_reply": "2025-08-20T21:53:10.673622Z",
     "shell.execute_reply.started": "2025-08-20T21:53:10.514775Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.28981135980903"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4**(10/4.6)*20*0.8/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "77f2b254-7d71-4532-9348-3e865a8fae07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T21:53:34.852632Z",
     "iopub.status.busy": "2025-08-20T21:53:34.847289Z",
     "iopub.status.idle": "2025-08-20T21:53:34.896704Z",
     "shell.execute_reply": "2025-08-20T21:53:34.891221Z",
     "shell.execute_reply.started": "2025-08-20T21:53:34.852401Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.362264199761288"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4**(10/4.6)*20*1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd89bd2-9efd-4e47-a4eb-cbfd98a20f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET TABLE TO EXPLORE GEN_ALG\n",
    "# with number_row as (\n",
    "#     select\n",
    "#         entity_id,\n",
    "#         name,\n",
    "#         entity_name,\n",
    "#         latitude,\n",
    "#         longitude,\n",
    "#         wait_time,\n",
    "#         extracted_at_time,\n",
    "#         rating,\n",
    "#         row_number() over (partition by entity_id order by extracted_at_time) as rn\n",
    "#     from themeparkwizard.predictions_table pt\n",
    "#     left join themeparkwizard.dim_park_entity dpe using(entity_id)\n",
    "#     where was_predicted = 1\n",
    "# ),\n",
    "# avg_by_entity AS (\n",
    "#     SELECT\n",
    "#         entity_id,\n",
    "#         AVG(avg_standby_waittime) as alltime_avg_waittime\n",
    "#     FROM themeparkwizard.agg_avg_time\n",
    "#     GROUP BY 1\n",
    "# ),\n",
    "# first_group as (\n",
    "#     select extracted_at_time,\n",
    "#            wait_time,\n",
    "#            entity_name,\n",
    "#            latitude,\n",
    "#            longitude,\n",
    "#            name,\n",
    "#            rating,\n",
    "#            alltime_avg_waittime\n",
    "#     from number_row\n",
    "#              left join avg_by_entity\n",
    "#                        using (entity_id)\n",
    "#     where rn <> 1\n",
    "#     order by extracted_at_time, entity_name\n",
    "# )\n",
    "# select\n",
    "#     extracted_at_time,\n",
    "#     name,\n",
    "#     a.entity_name as src_node,\n",
    "#     b.entity_name as dst_node,\n",
    "#     SQRT(POWER((b.latitude - a.latitude)*111, 2) + POWER((b.longitude - a.longitude)*111, 2)) AS euclidean_distance,\n",
    "#     b.wait_time,\n",
    "#     b.alltime_avg_waittime,\n",
    "#     b.rating\n",
    "# from first_group a\n",
    "# full join first_group b\n",
    "# using(extracted_at_time, name)\n",
    "# where a.entity_name <> b.entity_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
