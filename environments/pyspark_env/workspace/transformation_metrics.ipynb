{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21566461-a179-4df7-8621-803834840d6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:50:29.265571Z",
     "iopub.status.busy": "2025-08-14T01:50:29.262146Z",
     "iopub.status.idle": "2025-08-14T01:50:29.284097Z",
     "shell.execute_reply": "2025-08-14T01:50:29.280097Z",
     "shell.execute_reply.started": "2025-08-14T01:50:29.265255Z"
    }
   },
   "source": [
    "# Transformation metrics\n",
    "This script is responsible create all metrics necessary and upload them to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df6a8586-716f-479a-9ed2-505e0361fc87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T04:10:35.100013Z",
     "iopub.status.busy": "2025-09-03T04:10:35.099469Z",
     "iopub.status.idle": "2025-09-03T04:10:35.141726Z",
     "shell.execute_reply": "2025-09-03T04:10:35.139808Z",
     "shell.execute_reply.started": "2025-09-03T04:10:35.099970Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bf447ba-f45d-4867-a2c2-6760b2cdbaf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T04:10:35.146924Z",
     "iopub.status.busy": "2025-09-03T04:10:35.146287Z",
     "iopub.status.idle": "2025-09-03T04:10:48.272131Z",
     "shell.execute_reply": "2025-09-03T04:10:48.269466Z",
     "shell.execute_reply.started": "2025-09-03T04:10:35.146880Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.functions import explode, from_unixtime, col, to_date, sum, avg, udf\n",
    "from pyspark.sql.types import DateType, TimestampType, StructType, DoubleType, StructField, StringType, DayTimeIntervalType, IntegerType, LongType\n",
    "from prophet import Prophet\n",
    "\n",
    "from glob import glob\n",
    "import requests\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import locale\n",
    "import os\n",
    "\n",
    "DB_URL = \"jdbc:postgresql://postgres:5432/themeparkwizard\"\n",
    "PROPERTIES_CUSTOM = {\"user\": os.environ['POSTGRES_USER'],\"password\": os.environ['POSTGRES_PASSWORD'], \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MetricBuilder\") \\\n",
    "    .config(\"spark.jars\", \"jars/postgresql-42.7.7.jar\") \\\n",
    "    .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d074a6c-dac5-42f3-83d8-3cc02c3e8b92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T04:10:48.278016Z",
     "iopub.status.busy": "2025-09-03T04:10:48.275842Z",
     "iopub.status.idle": "2025-09-03T04:10:49.826627Z",
     "shell.execute_reply": "2025-09-03T04:10:49.820220Z",
     "shell.execute_reply.started": "2025-09-03T04:10:48.277952Z"
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=TimestampType())\n",
    "def min_hour(data):\n",
    "    if isinstance(data, list):\n",
    "        for ee in data:\n",
    "            if ee.type == 'Early Entry':\n",
    "                return ee.startTime\n",
    "\n",
    "@udf(returnType=TimestampType())\n",
    "def max_hour(data):\n",
    "    if isinstance(data, list):\n",
    "        for op in data:\n",
    "            if op.type == 'Operating':\n",
    "                return op.endTime\n",
    "\n",
    "def save_into_postgres(df, table, mode):\n",
    "    df.write.jdbc(url=DB_URL, table=table, mode=mode, properties=PROPERTIES_CUSTOM)\n",
    "\n",
    "def agg_avg_time_compute():\n",
    "    print('Computing agg_avg_time...')\n",
    "    result_avg = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        extracted_date,\n",
    "        id as entity_id, \n",
    "        AVG(queue.STANDBY.waitTime) AS avg_standby_waittime\n",
    "    FROM datalake_table\n",
    "    WHERE entity_type = 'ATTRACTION' AND queue.STANDBY.waitTime is not null\n",
    "    GROUP BY 1, 2\n",
    "    ORDER BY 1\n",
    "    \"\"\")\n",
    "    result_avg.printSchema()\n",
    "    save_into_postgres(result_avg, \"themeparkwizard.agg_avg_time\", 'append')\n",
    "    # result_avg.write.jdbc(url=DB_URL, table=\"themeparkwizard.agg_avg_time\", mode='append', properties=PROPERTIES_CUSTOM)\n",
    "\n",
    "def operating_ratio_compute(): \n",
    "    print('Computing operating...')\n",
    "    result_ratio = spark.sql(\"\"\"\n",
    "    WITH table_status AS (\n",
    "        SELECT\n",
    "            extracted_at_time,\n",
    "            lead(extracted_at_time, 1) OVER (PARTITION BY id ORDER BY extracted_at_time) as next_time,\n",
    "            id as entity_id,\n",
    "            status,\n",
    "            lead(status, 1) OVER (PARTITION BY id ORDER BY extracted_at_time) as next_status\n",
    "        FROM datalake_table\n",
    "        WHERE entity_type = 'ATTRACTION'\n",
    "        ORDER BY 1,3\n",
    "        ), interval_by_status AS (\n",
    "        SELECT \n",
    "            entity_id,\n",
    "            (unix_timestamp(next_time) - unix_timestamp(extracted_at_time)) as time_passed,\n",
    "            CASE\n",
    "            WHEN status = 'OPERATING' THEN 1\n",
    "            ELSE 0\n",
    "            END as whole_status\n",
    "        FROM table_status\n",
    "        )\n",
    "        SELECT \n",
    "            entity_id,\n",
    "            sum(time_passed) as time_by_status,\n",
    "            whole_status\n",
    "        FROM interval_by_status\n",
    "        GROUP BY 1,3\n",
    "    \"\"\")\n",
    "    result_ratio.printSchema()\n",
    "    save_into_postgres(result_ratio, \"themeparkwizard.operating_ratio\", 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abf6ff69-b9cf-4a94-9fd3-482e36b75cbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T04:10:49.832129Z",
     "iopub.status.busy": "2025-09-03T04:10:49.830470Z",
     "iopub.status.idle": "2025-09-03T04:11:04.396925Z",
     "shell.execute_reply": "2025-09-03T04:11:04.393641Z",
     "shell.execute_reply.started": "2025-09-03T04:10:49.832049Z"
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"extracted_date\", DateType(), False),\n",
    "    StructField(\"entity_id\", StringType(), False),\n",
    "    StructField(\"avg_standby_waittime\", DoubleType(), False),\n",
    "])\n",
    "save_into_postgres(spark.createDataFrame([], schema), \"themeparkwizard.agg_avg_time\", 'overwrite')\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"entity_id\", StringType(), False),\n",
    "    StructField(\"time_by_status\", LongType(), False),\n",
    "    StructField(\"whole_status\", IntegerType(), False),\n",
    "])\n",
    "save_into_postgres(spark.createDataFrame([], schema), \"themeparkwizard.operating_ratio\", 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "240ac968-85c6-43fd-a8bb-71f1cb85feb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T04:11:04.404057Z",
     "iopub.status.busy": "2025-09-03T04:11:04.401112Z",
     "iopub.status.idle": "2025-09-03T04:11:07.216573Z",
     "shell.execute_reply": "2025-09-03T04:11:07.214055Z",
     "shell.execute_reply.started": "2025-09-03T04:11:04.403960Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load dim_park_entity\n",
    "df_parks = spark.read.json('general_schemas_tables/park_by_entity_meta_new.json')\n",
    "df_parks.write.jdbc(url=DB_URL, table=f\"themeparkwizard.dim_park_entity\", mode='overwrite', properties=PROPERTIES_CUSTOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cdad2a-d5c9-4265-a2de-c3e1ab1230bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T04:11:07.220219Z",
     "iopub.status.busy": "2025-09-03T04:11:07.219676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming datalake_layer/animal_kingdom ...\n",
      "Computing agg_avg_time...\n",
      "root\n",
      " |-- extracted_date: date (nullable = true)\n",
      " |-- entity_id: string (nullable = true)\n",
      " |-- avg_standby_waittime: double (nullable = true)\n",
      "\n",
      "Computing operating...\n",
      "root\n",
      " |-- entity_id: string (nullable = true)\n",
      " |-- time_by_status: long (nullable = true)\n",
      " |-- whole_status: integer (nullable = false)\n",
      "\n",
      "Transforming datalake_layer/epcot ...\n",
      "Computing agg_avg_time...\n",
      "root\n",
      " |-- extracted_date: date (nullable = true)\n",
      " |-- entity_id: string (nullable = true)\n",
      " |-- avg_standby_waittime: double (nullable = true)\n",
      "\n",
      "Computing operating...\n",
      "root\n",
      " |-- entity_id: string (nullable = true)\n",
      " |-- time_by_status: long (nullable = true)\n",
      " |-- whole_status: integer (nullable = false)\n",
      "\n",
      "Transforming datalake_layer/hollywood_studios ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for path in glob('datalake_layer/*'):\n",
    "    print(f'Transforming {path} ...')\n",
    "    df_dl = spark.read.orc(path).cache()\n",
    "    # df_dl_working_hour = df_dl.withColumn('start_time', min_hour(col('operatingHours')))\\\n",
    "    #                 .withColumn('end_time', max_hour(col('operatingHours')))\\\n",
    "    #                 .filter(col('extracted_at_time').between(col('start_time'), col('end_time')))\n",
    "    # df_dl_working_hour.createOrReplaceTempView('datalake_table_working_hour')\n",
    "    df_dl.createOrReplaceTempView('datalake_table')\n",
    "    agg_avg_time_compute()\n",
    "    operating_ratio_compute()\n",
    "# df_dl.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff19d4fc-1fea-4205-87f5-26fb35a89c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    WITH table_status AS (\n",
    "        SELECT\n",
    "            extracted_at_time,\n",
    "            lead(extracted_at_time, 1) OVER (PARTITION BY id ORDER BY extracted_at_time) as next_time,\n",
    "            id as entity_id,\n",
    "            status,\n",
    "            lead(status, 1) OVER (PARTITION BY id ORDER BY extracted_at_time) as next_status\n",
    "        FROM datalake_table\n",
    "        WHERE entity_type = 'ATTRACTION'\n",
    "        ORDER BY 1,3\n",
    "        ), interval_by_status AS (\n",
    "        SELECT \n",
    "            entity_id,\n",
    "            (unix_timestamp(next_time) - unix_timestamp(extracted_at_time)) as time_passed,\n",
    "            CASE\n",
    "            WHEN status = 'OPERATING' THEN 1\n",
    "            ELSE 0\n",
    "            END as whole_status\n",
    "        FROM table_status\n",
    "        )\n",
    "        SELECT \n",
    "            entity_id,\n",
    "            sum(time_passed) as time_by_status,\n",
    "            whole_status\n",
    "        FROM interval_by_status\n",
    "        GROUP BY 1,3\n",
    "    \"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32825e06-7d7e-4e0a-b656-c37a3c184574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2605d272-d09b-4ced-8614-c2e0a36e74b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
